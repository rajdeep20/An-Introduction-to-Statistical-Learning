rm(list=ls())
library(ISLR)
library(class)
library(tree)
library(randomForest)
library(gbm)
set.seed(1)

attach(Auto)
names(Auto)
mpg01=rep(0,nrow(Auto))
mpg01[mpg>median(mpg)]=1
Auto=cbind(Auto,mpg01)
#fix(Auto)

#train test split
train=sample(1:nrow(Auto),floor(nrow(Auto))*0.8)
Auto=Auto[,c(2:5,10)]

#knn with k=3
auto.train=as.matrix(Auto[train,])
auto.test=as.matrix(Auto[-train,])
mpg01.train=as.vector(Auto$mpg01[train])
mpg01.test=as.vector(Auto$mpg01[-train])
knn.pred=knn(auto.train,auto.test,mpg01.train,k=3)
tab=table(knn.pred,mpg01.test)
tab
mean(knn.pred==mpg01.test)

#logistic regression
logistic.auto=glm(mpg01~.,data=Auto,subset=train,family=binomial)
logistic.probs=predict(logistic.auto,newdata=Auto[-train,],type="response")
logistic.pred=rep(0,nrow(Auto[-train,]))
logistic.pred[logistic.probs>0.5]=1
tab=table(logistic.pred,mpg01.test)
tab
mean(logistic.pred==mpg01.test)


#bagging
p=ncol(Auto)-1
Auto$mpg01=as.factor(mpg01)
bag.auto=randomForest(mpg01~.,data=Auto,subset=train,mtry=p,importance=TRUE)
bag.auto
bag.pred=predict(bag.auto,newdata=auto.test,type="response")
tab=table(bag.pred,mpg01.test)
tab
mean(bag.pred==mpg01.test)

#random forest
rf.auto=randomForest(mpg01~.,data=Auto,subset=train,mtry=sqrt(p),importance=TRUE)
rf.auto
rf.pred=predict(rf.auto,newdata=auto.test,type="response")
tab=table(rf.pred,mpg01.test)
tab
mean(rf.pred==mpg01.test)

#boosting
Auto$mpg01=as.numeric(mpg01)
boost.auto=gbm(mpg01~.,data=Auto[train,],distribution="bernoulli",n.trees=1000,interaction.depth=4)
boost.prob=predict(boost.auto,newdata=Auto[-train,],type="response")
boost.pred=rep(0,nrow(Auto[-train,]))
boost.pred[boost.prob>0.5]=1
boost.pred=as.factor(boost.pred)
tab=table(boost.pred,mpg01.test)
tab
mean(boost.pred==mpg01.test)

